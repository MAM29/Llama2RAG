{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find torch installation command for your machine at https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain einops accelerate transformers bitsandbytes scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install xformers sentencepiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-index==0.7.21 llama_hub==0.0.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install llama-index --upgrade --no-cache-dir --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -U llama-index-readers-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformer classes for generaiton\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "# Import torch for datatype attributes \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable to hold llama2 weights naming \n",
    "name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# Set auth token variable from hugging face \n",
    "auth_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, \n",
    "    cache_dir='./model/', use_auth_token=auth_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = AutoModelForCausalLM.from_pretrained(name, \n",
    "    cache_dir='./model/', use_auth_token=auth_token, torch_dtype=torch.float16, \n",
    "    rope_scaling={\"type\": \"dynamic\", \"factor\": 2}, load_in_8bit=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the prompt wrapper...but for llama index\n",
    "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
    "# Create a system prompt \n",
    "system_prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "\n",
    " You are now a carbon footprint analyst. Your job is to reason about the carbon footprint of \"{product_name}\" \n",
    " based off its components. For each part of the calculation, explain how you came to that conclusion. Only use \n",
    " factual data for your computations and do not make assumptions. Do not use information about any other product\n",
    " other than \"{product_name}\" to perform your computations. If you cannot compute the carbon footprint \n",
    " from known information, return \"None\".<</SYS>>\n",
    "\"\"\"\n",
    "# Throw together the query wrapper\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"{query_str} [/INST]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the query prompt\n",
    "query_wrapper_prompt.format(query_str='hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the llama index HF Wrapper\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "# Create a HF LLM using the llama index wrapper \n",
    "llm = HuggingFaceLLM(context_window=4096,\n",
    "                    max_new_tokens=1024,\n",
    "                    system_prompt=system_prompt,\n",
    "                    query_wrapper_prompt=query_wrapper_prompt,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in embeddings wrapper\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "# Bring in HF embeddings - need these to represent document chunks\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and dl embeddings instance  \n",
    "embeddings=LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in stuff to change settings\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish llama_index model settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embeddings\n",
    "Settings.chunk_size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import deps to load documents \n",
    "from llama_index.core import VectorStoreIndex, download_loader\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.chains import RetrievalQA\n",
    "from llama_index.core import Document\n",
    "\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt user to enter question\n",
    "user_question = input(\"User:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store csv data into SQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/examples/index_structs/struct_indices/SQLIndexDemo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Database Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    select,\n",
    "    Float,\n",
    ")\n",
    "from llama_index.core import SQLDatabase\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create city SQL table\n",
    "table_name = \"device_specs\"\n",
    "device_specs_table = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"Carbon Filename\", String(16)),\n",
    "    Column(\"Company\", String(16)),\n",
    "    Column(\"Device\", String(16)),\n",
    "    Column(\"Commercial_Name\", String(16), primary_key=True),\n",
    "    Column(\"PCF\", Integer),\n",
    "    Column(\"Manufacturing %\", Float),\n",
    "    Column(\"Chassis & Assembly %\", Float),\n",
    "    Column(\"Hard Drive %\", Float),\n",
    "    Column(\"SSD %\", Float),\n",
    "    Column(\"Power Supply %\", Float),\n",
    "    Column(\"Battery %\", Float),\n",
    "    Column(\"Mainboard and Other Boards %\", Float),\n",
    "    Column(\"Display %\", Float),\n",
    "    Column(\"Packaging %\", Float),\n",
    "    Column(\"Manufacturing Emissions\", Float),\n",
    "    Column(\"Chassis & Assembly Emissions\", Float),\n",
    "    Column(\"Hard Drive Emissions\", Float),\n",
    "    Column(\"SSD Emissions\", Float),\n",
    "    Column(\"Power Supply Emissions\", Float),\n",
    "    Column(\"Battery Emissions\", Float),\n",
    "    Column(\"Mainboard and Other Boards Emissions\", Float),\n",
    "    Column(\"Display Emissions\", Float),\n",
    "    Column(\"Packaging Emissions\", Float),\n",
    "    Column(\"Other Emissions\", Float),\n",
    "    Column(\"Specs Filename\", String(16)),\n",
    "    Column(\"Category\", String(16)),\n",
    "    Column(\"Processor Cores\", Float),\n",
    "    Column(\"RAM\", Float),\n",
    "    Column(\"SSD\", Float),\n",
    "    Column(\"HDD\", Float),\n",
    "    Column(\"Power\", Float),\n",
    "    Column(\"Display\", Float),\n",
    "    Column(\"Weight\", Float)\n",
    ")\n",
    "metadata_obj.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add csv to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import insert\n",
    "\n",
    "sql_database = SQLDatabase(engine, include_tables=[table_name])\n",
    "\n",
    "# Convert the DataFrame into a list of dictionaries\n",
    "rows = df.to_dict('records')\n",
    "\n",
    "# Add csv to SQL database\n",
    "for row in rows:\n",
    "    stmt = insert(device_specs_table).values(**row)\n",
    "    with engine.begin() as connection:\n",
    "        cursor = connection.execute(stmt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View database items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# view current table\n",
    "stmt = select(\n",
    "    device_specs_table.c.Company,\n",
    "    device_specs_table.c.Device,\n",
    "    device_specs_table.c[\"Commercial_Name\"],\n",
    "    device_specs_table.c.PCF,\n",
    ").select_from(device_specs_table)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    results = connection.execute(stmt).fetchall()\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "# Surround user_question with quotes and add wildcard character % for partial matching\n",
    "search_term = f\"'%{user_question}%'\"\n",
    "\n",
    "# Use text() to create a SQL expression\n",
    "sql_query = text(f\"SELECT * FROM device_specs WHERE `Commercial_Name` LIKE {search_term}\")\n",
    "\n",
    "# Execute the query\n",
    "with engine.connect() as con:\n",
    "    rows = con.execute(sql_query)\n",
    "    for row in rows:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "from llama_index.core.retrievers import NLSQLRetriever\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "# default retrieval (return_raw=False)\n",
    "nl_sql_retriever = NLSQLRetriever(\n",
    "    sql_database, tables=[table_name], return_raw=False\n",
    ")\n",
    "\n",
    "results = nl_sql_retriever.retrieve(\n",
    "    user_question\n",
    ")\n",
    "\n",
    "# NOTE: all the content is in the metadata\n",
    "for n in results:\n",
    "    display_source_node(n, show_source_metadata=True)\n",
    "\n",
    "#response = query_engine.query(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(nl_sql_retriever, streaming=True)\n",
    "\n",
    "response = query_engine.query(\n",
    "    user_question\n",
    ")\n",
    "response.print_response_stream()\n",
    "\n",
    "#print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
